<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Publications - Efstathia Soufleri</title>
  <meta name="description" content="Academic webpage of Efstathia Soufleri">
  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="/publications/">
  <link rel="shortcut icon" type ="image/x-icon" href="/favicon.ico">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">

  <link rel="preconnect" href="https://player.vimeo.com">
  <link rel="preconnect" href="https://i.vimeocdn.com">
  <link rel="preconnect" href="https://f.vimeocdn.com">



<!-- Google Analytics (original) -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', '', 'auto');
  ga('send', 'pageview');

</script>

<!-- Global site tag (gtag.js) - Google Analytics 4 -->
<script async src="https://www.googletagmanager.com/gtag/js?id="></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', '');
</script>

<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','');</script>
<!-- End Google Tag Manager -->



</head>


  <body>

    <!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id="
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->

<nav class="navbar sticky-top navbar-expand-md navbar-dark bg-dark">
    <a class="navbar-brand" href="/">
     <img src="/favicon.ico" width="30" height="30" style="margin-right:5px" class="d-inline-block align-top" alt="">
      Efstathia Soufleri
    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarColor02" aria-controls="navbarColor02" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse" id="navbarColor02">
        <ul class="navbar-nav mr-auto">
        <ul class="navbar-nav">
          <li class="nav-item">
              <a class="nav-link" href="/">Home</a>
          </li> 
          
           <li class="nav-item">
            <a class="nav-link" href="/about">About</a>
           </li> 
          
           <li class="nav-item">
            <a class="nav-link" href="/publications">Publications</a>
           </li> 
          
           <li class="nav-item">
            <a class="nav-link" href="/research">Research</a>
           </li> 
          
        </ul>
  </div>
</nav>



    <div class="container-fluid">
      <div class="row">
        <div id="gridid" class="col-sm-12 col-xs-12">
  <style>
.jumbotron{
    padding:3%;
    padding-bottom:10px;
    padding-top:10px;
    margin-top:10px;
    margin-bottom:30px;
}
</style>

<div class="jumbotron">
  <h3 id="refereed-journal-articles">Refereed journal articles</h3>
  <ol class="bibliography" reversed="reversed"><li><style>
.btn{
    margin-bottom:5px;
    padding-top:0px;
    padding-bottom:0px;
    padding-left:15px;
    padding-right:15px;
    height:20px:
}
pre{
    white-space: pre-wrap;  
    white-space: -moz-pre-wrap; 
    white-space: -pre-wrap; 
    white-space: -o-pre-wrap; 
    word-wrap: break-word; 
    width:100%; overflow-x:auto;
}
</style>


<!-- <div class="text-justify"><span id="soufleridp">Soufleri, E., Ravikumar, D., &amp; Roy, K. (2024). DP-ImgSyn: Dataset Alignment for Obfuscated, Differentially Private Image Synthesis. <i>TMLR</i>.</span></div> -->

<div class="text-justify"><span id="soufleridp"><b><b>Soufleri, E.</b></b>, Ravikumar, D., &amp; Roy, K. (2024). DP-ImgSyn: Dataset Alignment for Obfuscated, Differentially Private Image Synthesis. <i>TMLR</i>.</span></div>
<!-- You can use the below to make your name bold -->
<!-- <span id="soufleridp"><b><b>Soufleri, E.</b></b>, Ravikumar, D., &amp; Roy, K. (2024). DP-ImgSyn: Dataset Alignment for Obfuscated, Differentially Private Image Synthesis. <i>TMLR</i>.</span> -->





<a href="/papers/2144_dp_imgsyn_dataset_alignment_fo.pdf" target="_blank"><button class="btn btn-success btm-sm">PDF</button></a>




<!-- 
<button class="btn btn-danger btm-sm"  onclick="toggleBibtexsoufleridp()">BIB</button>
 -->


<button class="btn btn-warning btm-sm" onclick="toggleAbstractsoufleridp()">ABSTRACT</button>



<div id="asoufleridp" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>@article{soufleridp,
  title = {DP-ImgSyn: Dataset Alignment for Obfuscated, Differentially Private Image Synthesis},
  author = {Soufleri, Efstathia and Ravikumar, Deepak and Roy, Kaushik},
  journal = {TMLR},
  year = {2024},
  publisher = {IEEE},
  file = {2144_dp_imgsyn_dataset_alignment_fo.pdf}
}
</pre>
</div>


<div id="bsoufleridp" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>The availability of abundant data has catalyzed the expansion of deep learning vision algo- rithms. However, certain vision datasets cannot be publicly released due to privacy reasons. Releasing synthetic images instead of private images is a common approach to overcome this issue. A popular method to generate synthetic images is using Generative Adversar- ial Networks (GANs) with Differential Privacy (DP) guarantees. However, GAN-generated synthetic images are visually similar to private images. This is a severe limitation, par- ticularly when the private dataset depicts visually sensitive and disturbing content. To address this, we propose a non-generative framework, Differentially Private Image Synthesis (DP-ImgSyn), to generate and release synthetic images for image classification tasks. These synthetic images: (1) have DP guarantees, (2) retain the utility of the private images, i.e., a model trained using synthetic images results in similar accuracy as a model trained on pri- vate images, (3) the synthetic images are visually dissimilar to private images. DP-ImgSyn consists of the following steps: First, a teacher model is trained on the private images using a DP training algorithm. Second, public images are used as initialization for the synthetic im- ages which are optimized to align them with the private images. The optimization uses the teacher network’s batch normalization layer statistics (mean, standard deviation) to inject information about the private images into the synthetic images. Third, the synthetic images and their soft labels, obtained from the teacher model, are released and can be deployed for neural network training on image classification tasks. Our experiments on various image classification datasets show that when using similar DP training mechanisms, our framework performs better than generative techniques (up to ≈ 20% in terms of image classification accuracy).</pre>
</div>

<script>
function toggleBibtexsoufleridp(parameter) {
    var x= document.getElementById('asoufleridp');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
function toggleAbstractsoufleridp(parameter) {
    var x= document.getElementById('bsoufleridp');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><style>
.btn{
    margin-bottom:5px;
    padding-top:0px;
    padding-bottom:0px;
    padding-left:15px;
    padding-right:15px;
    height:20px:
}
pre{
    white-space: pre-wrap;  
    white-space: -moz-pre-wrap; 
    white-space: -pre-wrap; 
    white-space: -o-pre-wrap; 
    word-wrap: break-word; 
    width:100%; overflow-x:auto;
}
</style>


<!-- <div class="text-justify"><span id="soufleri2021network">Soufleri, E., &amp; Roy, K. (2021). Network Compression via Mixed Precision Quantization Using a Multi-Layer Perceptron for the Bit-Width Allocation. <i>IEEE Access</i>, <i>9</i>, 135059–135068.</span></div> -->

<div class="text-justify"><span id="soufleri2021network"><b><b>Soufleri, E.</b></b>, &amp; Roy, K. (2021). Network Compression via Mixed Precision Quantization Using a Multi-Layer Perceptron for the Bit-Width Allocation. <i>IEEE Access</i>, <i>9</i>, 135059–135068.</span></div>
<!-- You can use the below to make your name bold -->
<!-- <span id="soufleri2021network"><b><b>Soufleri, E.</b></b>, &amp; Roy, K. (2021). Network Compression via Mixed Precision Quantization Using a Multi-Layer Perceptron for the Bit-Width Allocation. <i>IEEE Access</i>, <i>9</i>, 135059–135068.</span> -->





<a href="/papers/soufleri2021network.pdf" target="_blank"><button class="btn btn-success btm-sm">PDF</button></a>




<a href="http://doi.org/10.1109/ACCESS.2021.3116418" target="_blank"><button class="btn btn-primary btm-sm">DOI</button></a>



<!-- 
<button class="btn btn-danger btm-sm"  onclick="toggleBibtexsoufleri2021network()">BIB</button>
 -->


<button class="btn btn-warning btm-sm" onclick="toggleAbstractsoufleri2021network()">ABSTRACT</button>



<div id="asoufleri2021network" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>@article{soufleri2021network,
  title = {Network Compression via Mixed Precision Quantization Using a Multi-Layer Perceptron for the Bit-Width Allocation},
  author = {Soufleri, Efstathia and Roy, Kaushik},
  journal = {IEEE Access},
  volume = {9},
  pages = {135059--135068},
  year = {2021},
  publisher = {IEEE},
  doi = {10.1109/ACCESS.2021.3116418},
  file = {soufleri2021network.pdf}
}
</pre>
</div>


<div id="bsoufleri2021network" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>Deep Neural Networks (DNNs) are a powerful tool for solving complex tasks in many application domains. The high performance of DNNs demands significant computational resources, which might not always be available. Network quantization with mixed-precision across the layers can alleviate this high demand. However, determining layer-wise optimal bit-widths is non-trivial, as the search space is exponential. This article proposes a novel technique for allocating layer-wise bit-widths for a DNN using a multi-layer perceptron (MLP). The Kullback-Leibler (KL) divergence of the softmax outputs between the quantized and full precision network is used as the metric to quantify the quantization quality. We explore the relationship between the KL-divergence and the network size, and from our experiments observe that more aggressive quantization leads to higher divergence, and vice versa. The MLP is trained with layer-wise bit-widths as labels and their corresponding KL-divergence as the input. The MLP training set, i.e. the pairs of the layer-wise bit-widths and their corresponding KL-divergence, is collected using a Monte Carlo sampling of the exponential search space. We introduce a penalty term in the loss to ensure that the MLP learns to predict bit-widths resulting in the smallest network size. We show that the layer-wise bit-width predictions from the trained MLP result in reduced network size without degrading accuracy while achieving better or comparable results with SOTA work but with less computational overhead. Our method achieves up to 6x, 4x, 4x compression on VGG16, ResNet50, and GoogLeNet respectively, with no accuracy drop compared to the original full precision pretrained model, on the ImageNet dataset.</pre>
</div>

<script>
function toggleBibtexsoufleri2021network(parameter) {
    var x= document.getElementById('asoufleri2021network');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
function toggleAbstractsoufleri2021network(parameter) {
    var x= document.getElementById('bsoufleri2021network');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li></ol>
</div>

<div class="jumbotron">
  <h3 id="refereed-conference-proceedings">Refereed conference proceedings</h3>
  <ol class="bibliography" reversed="reversed"><li><style>
.btn{
    margin-bottom:5px;
    padding-top:0px;
    padding-bottom:0px;
    padding-left:15px;
    padding-right:15px;
    height:20px:
}
pre{
    white-space: pre-wrap;  
    white-space: -moz-pre-wrap; 
    white-space: -pre-wrap; 
    white-space: -o-pre-wrap; 
    word-wrap: break-word; 
    width:100%; overflow-x:auto;
}
</style>


<!-- <div class="text-justify"><span id="ravikumar2025curvature">Ravikumar, D., Soufleri, E., &amp; Roy, K. (2025). Curvature clues: Decoding deep learning privacy with input loss curvature. <i>Advances in Neural Information Processing Systems</i>, <i>37</i>, 20003–20030.</span></div> -->

<div class="text-justify"><span id="ravikumar2025curvature">Ravikumar, D., <b><b>Soufleri, E.</b></b>, &amp; Roy, K. (2025). Curvature clues: Decoding deep learning privacy with input loss curvature. <i>Advances in Neural Information Processing Systems</i>, <i>37</i>, 20003–20030.</span></div>
<!-- You can use the below to make your name bold -->
<!-- <span id="ravikumar2025curvature">Ravikumar, D., <b><b>Soufleri, E.</b></b>, &amp; Roy, K. (2025). Curvature clues: Decoding deep learning privacy with input loss curvature. <i>Advances in Neural Information Processing Systems</i>, <i>37</i>, 20003–20030.</span> -->





<a href="/papers/NeurIPS-2024-curvature-clues-decoding-deep-learning-privacy-with-input-loss-curvature-Paper-Conference.pdf" target="_blank"><button class="btn btn-success btm-sm">PDF</button></a>




<!-- 
<button class="btn btn-danger btm-sm"  onclick="toggleBibtexravikumar2025curvature()">BIB</button>
 -->


<button class="btn btn-warning btm-sm" onclick="toggleAbstractravikumar2025curvature()">ABSTRACT</button>



<div id="aravikumar2025curvature" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>@inproceedings{ravikumar2025curvature,
  title = {Curvature clues: Decoding deep learning privacy with input loss curvature},
  author = {Ravikumar, Deepak and Soufleri, Efstathia and Roy, Kaushik},
  journal = {Advances in Neural Information Processing Systems},
  volume = {37},
  pages = {20003--20030},
  year = {2025},
  file = {NeurIPS-2024-curvature-clues-decoding-deep-learning-privacy-with-input-loss-curvature-Paper-Conference.pdf}
}
</pre>
</div>


<div id="bravikumar2025curvature" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>In this paper, we explore the properties of loss curvature with respect to input data in deep neural networks. Curvature of loss with respect to input (termed input loss curvature) is the trace of the Hessian of the loss with respect to the input. We investigate how input loss curvature varies between train and test sets, and its implications for train-test distinguishability. We develop a theoretical framework that derives an upper bound on the train-test distinguishability based on privacy and the size of the training set. This novel insight fuels the development of a new black box membership inference attack utilizing input loss curvature. We validate our theoretical findings through experiments in computer vision classification tasks, demonstrating that input loss curvature surpasses existing methods in membership inference effectiveness. Our analysis highlights how the performance of membership inference attack (MIA) methods varies with the size of the training set, showing that curvature-based MIA outperforms other methods on sufficiently large datasets. This condition is often met by real datasets, as demonstrated by our results on CIFAR10, CIFAR100, and ImageNet. These findings not only advance our understanding of deep neural network behavior but also improve the ability to test privacy-preserving techniques in machine learning.</pre>
</div>

<script>
function toggleBibtexravikumar2025curvature(parameter) {
    var x= document.getElementById('aravikumar2025curvature');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
function toggleAbstractravikumar2025curvature(parameter) {
    var x= document.getElementById('bravikumar2025curvature');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><style>
.btn{
    margin-bottom:5px;
    padding-top:0px;
    padding-bottom:0px;
    padding-left:15px;
    padding-right:15px;
    height:20px:
}
pre{
    white-space: pre-wrap;  
    white-space: -moz-pre-wrap; 
    white-space: -pre-wrap; 
    white-space: -o-pre-wrap; 
    word-wrap: break-word; 
    width:100%; overflow-x:auto;
}
</style>


<!-- <div class="text-justify"><span id="ravikumar2024unveiling">Ravikumar, D., Soufleri, E., Hashemi, A., &amp; Roy, K. (2024). Unveiling Privacy, Memorization, and Input Curvature Links. <i>Proceedings of the 41st International Conference on Machine Learning</i>, 42192–42212.</span></div> -->

<div class="text-justify"><span id="ravikumar2024unveiling">Ravikumar, D., <b><b>Soufleri, E.</b></b>, Hashemi, A., &amp; Roy, K. (2024). Unveiling Privacy, Memorization, and Input Curvature Links. <i>Proceedings of the 41st International Conference on Machine Learning</i>, 42192–42212.</span></div>
<!-- You can use the below to make your name bold -->
<!-- <span id="ravikumar2024unveiling">Ravikumar, D., <b><b>Soufleri, E.</b></b>, Hashemi, A., &amp; Roy, K. (2024). Unveiling Privacy, Memorization, and Input Curvature Links. <i>Proceedings of the 41st International Conference on Machine Learning</i>, 42192–42212.</span> -->





<a href="/papers/ravikumar24a.pdf" target="_blank"><button class="btn btn-success btm-sm">PDF</button></a>




<!-- 
<button class="btn btn-danger btm-sm"  onclick="toggleBibtexravikumar2024unveiling()">BIB</button>
 -->


<button class="btn btn-warning btm-sm" onclick="toggleAbstractravikumar2024unveiling()">ABSTRACT</button>



<div id="aravikumar2024unveiling" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>@inproceedings{ravikumar2024unveiling,
  title = {Unveiling Privacy, Memorization, and Input Curvature Links},
  author = {Ravikumar, Deepak and Soufleri, Efstathia and Hashemi, Abolfazl and Roy, Kaushik},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  pages = {42192--42212},
  year = {2024},
  file = {ravikumar24a.pdf}
}
</pre>
</div>


<div id="bravikumar2024unveiling" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>Deep Neural Nets (DNNs) have become a pervasive tool for solving many emerging problems. However, they tend to overfit to and memorize the training set. Memorization is of keen interest since it is closely related to several concepts such as generalization, noisy learning, and privacy. To study memorization, Feldman (2019) proposed a formal score, however its computational requirements limit its practical use. Recent research has shown empirical evidence linking input loss curvature (measured by the trace of the loss Hessian w.r.t inputs) and memorization. It was shown to be 3 orders of magnitude more efficient than calculating the memorization score. However, there is a lack of theoretical understanding linking memorization with input loss curvature. In this paper, we not only investigate this connection but also extend our analysis to establish theoretical links between differential privacy, memorization, and input loss curvature. First, we derive an upper bound on memorization characterized by both differential privacy and input loss curvature. Second, we present a novel insight showing that input loss curvature is upper-bounded by the differential privacy parameter. Our theoretical findings are further empirically validated using deep models on CIFAR and ImageNet datasets, showing a strong correlation between our theoretical predictions and results observed in practice.</pre>
</div>

<script>
function toggleBibtexravikumar2024unveiling(parameter) {
    var x= document.getElementById('aravikumar2024unveiling');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
function toggleAbstractravikumar2024unveiling(parameter) {
    var x= document.getElementById('bravikumar2024unveiling');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><style>
.btn{
    margin-bottom:5px;
    padding-top:0px;
    padding-bottom:0px;
    padding-left:15px;
    padding-right:15px;
    height:20px:
}
pre{
    white-space: pre-wrap;  
    white-space: -moz-pre-wrap; 
    white-space: -pre-wrap; 
    white-space: -o-pre-wrap; 
    word-wrap: break-word; 
    width:100%; overflow-x:auto;
}
</style>


<!-- <div class="text-justify"><span id="kosta2022hyperx">Kosta, A., Soufleri, E., Chakraborty, I., Agrawal, A., Ankit, A., &amp; Roy, K. (2022). HyperX: A Hybrid RRAM-SRAM partitioned system for error recovery in memristive Xbars. <i>2022 Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE)</i>, 88–91.</span></div> -->

<div class="text-justify"><span id="kosta2022hyperx">Kosta, A., <b><b>Soufleri, E.</b></b>, Chakraborty, I., Agrawal, A., Ankit, A., &amp; Roy, K. (2022). HyperX: A Hybrid RRAM-SRAM partitioned system for error recovery in memristive Xbars. <i>2022 Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE)</i>, 88–91.</span></div>
<!-- You can use the below to make your name bold -->
<!-- <span id="kosta2022hyperx">Kosta, A., <b><b>Soufleri, E.</b></b>, Chakraborty, I., Agrawal, A., Ankit, A., &amp; Roy, K. (2022). HyperX: A Hybrid RRAM-SRAM partitioned system for error recovery in memristive Xbars. <i>2022 Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE)</i>, 88–91.</span> -->








<a href="http://doi.org/10.23919/DATE54114.2022.9774549" target="_blank"><button class="btn btn-primary btm-sm">DOI</button></a>



<!-- 
<button class="btn btn-danger btm-sm"  onclick="toggleBibtexkosta2022hyperx()">BIB</button>
 -->


<button class="btn btn-warning btm-sm" onclick="toggleAbstractkosta2022hyperx()">ABSTRACT</button>



<div id="akosta2022hyperx" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>@inproceedings{kosta2022hyperx,
  title = {HyperX: A Hybrid RRAM-SRAM partitioned system for error recovery in memristive Xbars},
  author = {Kosta, Adarsh and Soufleri, Efstathia and Chakraborty, Indranil and Agrawal, Amogh and Ankit, Aayush and Roy, Kaushik},
  booktitle = {2022 Design, Automation \&amp; Test in Europe Conference \&amp; Exhibition (DATE)},
  pages = {88--91},
  year = {2022},
  organization = {IEEE},
  doi = {10.23919/DATE54114.2022.9774549}
}
</pre>
</div>


<div id="bkosta2022hyperx" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>Memristive crossbars based on Non-volatile Memory (NVM) technologies such as RRAM, have recently shown great promise for accelerating Deep Neural Networks (DNNs). They achieve this by performing efficient Matrix-Vector-Multiplications (MVMs) while offering dense on-chip storage and minimal off-chip data movement. However, their analog nature of computing introduces functional errors due to non-ideal RRAM devices, significantly degrading the application accuracy. Further, RRAMs suffer from low endurance and high write costs, hindering on-chip trainability. To alleviate these limitations, we propose HyperX, a hybrid RRAM-SRAM system that leverages the complementary benefits of NVM and CMOS technologies. Our proposed system consists of a fixed RRAM block offering area and energy-efficient MVMs and an SRAM block enabling on-chip training to recover the accuracy drop due to the RRAM non-idealities. The improvements are reported in terms of energy and product of latency and area (ms×mm2) , termed as area-normalized latency. Our experiments on CIFAR datasets using ResNet-20 show up to 2.88 × and 10.1 × improvements in inference energy and area-normalized latency, respectively. In addition, for a transfer learning task from ImageNet to CIFAR datasets using ResNet-18, we observe up to 1.58 × and 4.48 × improvements in energy and area-normalized latency, respectively. These improvements are with respect to an all-SRAM baseline.</pre>
</div>

<script>
function toggleBibtexkosta2022hyperx(parameter) {
    var x= document.getElementById('akosta2022hyperx');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
function toggleAbstractkosta2022hyperx(parameter) {
    var x= document.getElementById('bkosta2022hyperx');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><style>
.btn{
    margin-bottom:5px;
    padding-top:0px;
    padding-bottom:0px;
    padding-left:15px;
    padding-right:15px;
    height:20px:
}
pre{
    white-space: pre-wrap;  
    white-space: -moz-pre-wrap; 
    white-space: -pre-wrap; 
    white-space: -o-pre-wrap; 
    word-wrap: break-word; 
    width:100%; overflow-x:auto;
}
</style>


<!-- <div class="text-justify"><span id="panda2019evaluating">Panda, P., Soufleri, E., &amp; Roy, K. (2019). Evaluating the Stability of Recurrent Neural Models during Training with Eigenvalue Spectra Analysis. <i>2019 International Joint Conference on Neural Networks (IJCNN)</i>, 1–8.</span></div> -->

<div class="text-justify"><span id="panda2019evaluating">Panda, P., <b><b>Soufleri, E.</b></b>, &amp; Roy, K. (2019). Evaluating the Stability of Recurrent Neural Models during Training with Eigenvalue Spectra Analysis. <i>2019 International Joint Conference on Neural Networks (IJCNN)</i>, 1–8.</span></div>
<!-- You can use the below to make your name bold -->
<!-- <span id="panda2019evaluating">Panda, P., <b><b>Soufleri, E.</b></b>, &amp; Roy, K. (2019). Evaluating the Stability of Recurrent Neural Models during Training with Eigenvalue Spectra Analysis. <i>2019 International Joint Conference on Neural Networks (IJCNN)</i>, 1–8.</span> -->





<a href="/papers/panda2019evaluating.pdf" target="_blank"><button class="btn btn-success btm-sm">PDF</button></a>




<a href="http://doi.org/10.1109/IJCNN.2019.8852181" target="_blank"><button class="btn btn-primary btm-sm">DOI</button></a>



<!-- 
<button class="btn btn-danger btm-sm"  onclick="toggleBibtexpanda2019evaluating()">BIB</button>
 -->


<button class="btn btn-warning btm-sm" onclick="toggleAbstractpanda2019evaluating()">ABSTRACT</button>



<div id="apanda2019evaluating" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>@inproceedings{panda2019evaluating,
  title = {Evaluating the Stability of Recurrent Neural Models during Training with Eigenvalue Spectra Analysis},
  author = {Panda, Priyadarshini and Soufleri, Efstathia and Roy, Kaushik},
  booktitle = {2019 International Joint Conference on Neural Networks (IJCNN)},
  pages = {1--8},
  year = {2019},
  organization = {IEEE},
  doi = {10.1109/IJCNN.2019.8852181},
  file = {panda2019evaluating.pdf}
}
</pre>
</div>


<div id="bpanda2019evaluating" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>We analyze the stability of recurrent networks, specifically, reservoir computing models during training by evaluating the eigenvalue spectra of the reservoir dynamics. To circumvent the instability arising in examining a closed loop reservoir system with feedback, we propose to break the closed loop system. Essentially, we unroll the reservoir dynamics over time while incorporating the feedback effects that preserve the overall temporal integrity of the system. We evaluate our methodology for fixed point and time varying targets with least squares regression and FORCE training, respectively. Our analysis establishes eigenvalue spectra (which is, shrinking of spectral circle as training progresses) as a valid and effective metric to gauge the convergence of training as well as the convergence of the chaotic activity of the reservoir toward stable states.</pre>
</div>

<script>
function toggleBibtexpanda2019evaluating(parameter) {
    var x= document.getElementById('apanda2019evaluating');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
function toggleAbstractpanda2019evaluating(parameter) {
    var x= document.getElementById('bpanda2019evaluating');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li></ol>
</div>


</div>

      </div>
    </div>

    <br/>
<section id="footer">
<div class="container-footer">
  <div class="panel-footer">
	  <div class="row">
		<div class="col-sm-4">
		    <h5>About</h5>	
            <p>Efstathia Soufleri<br/> Postdoctoral Researcher<br/> Machine Learning
</p>
		</div>

		<div class="col-sm-4">
		    <h5>Contact</h5>	
            <p><a href="mailto:esoufler@alumni.purdue.edu" target="_blank"><i class="fa fa-envelope fa-1x"></i> Contact Efstathia via email</a> <br/>
</p>
		</div>

		<div class="col-sm-4">
		    <h5>Coordinates</h5>	
            <p></p>
		</div>
	  </div>

      <center><p>&copy 2025 Efstathia Soufleri </p></center>
	</div>
  </div>
</div>

<script src="/assets/javascript/bootstrap/jquery.min.js"></script>
<script src="/assets/javascript/bootstrap/bootstrap.bundle.min.js"></script>


  </body>

</html>
